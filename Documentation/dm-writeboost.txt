dm-writeboost
=============
writeboost target provides log-structured caching.
It batches random writes into a big sequential write to a cache device.

It is like dm-cache but the difference is
that writeboost focuses on handling bursty writes and lifetime of SSD cache device.

Auxiliary PDF documents and Quick-start scripts are available in
https://github.com/akiradeveloper/dm-writeboost

Design
======
There are one foreground process
and 6 background daemons.

Foreground
----------
It accepts bios and put writes to RAM buffer.
When the buffer is full, it creates a "flush job" and queues it.

Background
----------
* Flush Daemon
Pop a flush job from a queue and executes it.

* Deferring ACK for barrier writes
Barrier flags such as REQ_FUA and REQ_FLUSH are handled lazily.
Immediately handling these bios badly slows down writeboost.
It surveils the bios with these flags and forcefully flash them
at worst case within `barrier_deadline_ms` period.

* Migration Daemon
It migrates, writes back cache data to backing store,
the data on the cache device in segment granurality.

If `allow_migrate` is true, it migrates without impending situation.
Being in impending situation is that there are no room in cache device
for writing further flush jobs.

Migration at a time is done batching `nr_max_batched_migration` segments at maximum.
Therefore, unlike existing I/O scheduler,
two dirty writes close in positional space but distant in time space can be merged.
writetboost is also a extension of I/O scheduler.

* Migration Modulator
Migration while the backing store is heavily loaded
grows the device queue longer and thus makes the situation ever worse.
This daemon modulates the migration by switching `allow_migrate`.

* Superblock Recorder
Superblock is a last sector of first 1MB region in cache device.
It contains what id of the segment lastly migrated.
This daemon periodically update the region every `update_record_interval` seconds.

* Dirty Synchronizer
This daemon forcefully makes all the dirty writes persistent
every `sync_interval` seconds.
Since writeboost correctly implements the bio semantics
writing the dirties out forcefully out of the main path is needless.
However, some user want to be on the safe side by enabling this.

Target Interface
================
All the operations are via dmsetup command.

Constructor
-----------
The first argument is the type of the buffer.
The following arguments alter along with it.

type
0: volatile ram buffer (DRAM)
1: non-volatile buffer backed by block device
2: non-volatile ram buffer (persistent memory)

writeboost 0 <backing dev> <cache dev>
           #optional args
           [segment_size_order val]
           [rambuf_pool_amount val]

backing dev        : slow device holding original data blocks.
cache dev          : fast device holding cached data and its metadata.
segment size order : the size of RAM buffer
                     1 << n (sectors), 4 <= n <= 10
                     default 7
rambuf pool amount : The amount of the RAM buffer pool (kB).
                     Too fewer amount may cause waiting for new buffer
                     to become available again.
                     But too much doesn't affect the performance.
                     default 2048

Note that cache device is re-formatted
if the first sector of the cache device is zeroed out.

Status
------
<cursor pos>
<# cache blocks>
<# segments>
<current id>
<lastly flushed id>
<lastly migrated id>
<# dirty cache blocks>
<stat info (w/r) x (hit/miss) x (on buffer?) x (fullsize?)>
<# tunable args>
[tunable params]* (see below "Messages")

Messages
--------
You can tune up the behavior of writeboost via message interface.

* barrier_deadline_ms (ms)
Default: 3
All the bios with barrier flags like REQ_FUA or REQ_FLUSH
are guaranteed to be acked within this deadline.

* allow_migrate (bool)
Default: 1
Set to 1 to start migration.

* enable_migration_modulator (bool) and
  migrate_threshold (%)
Default: 1 and 70
Set to 1 to run migration modulator.
Migration modulator surveils the load of backing store
and set the migration started when the load is
lower than the migrate_threshold.

* nr_max_batched_migration (int)
Default: 1MB / segment size
Number of segments to migrate simultaneously and atomically.
Set higher value to fully exploit the capacily of the backing store.
Single HDD is capable of processing 1MB/sec random writes so
the default value is set to 1MB / segment size.

* sync_interval (sec)
Default: 60
All the dirty writes are guaranteed to be persistent by this interval.

* update_record_interval (sec)
Default: 60
The superblock record is updated every update_record_interval seconds.

Example
=======
dd if=/dev/zero of=${CACHE} bs=512 count=1 oflag=direct
sz=`blockdev --getsize ${BACKING}`
dmsetup create writeboost-vol --table "0 ${sz} 0 writeboost ${BACKING} {CACHE} 4 \
                                       rambuf_pool_amount 8192 segment_size_order 8"
